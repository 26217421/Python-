{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from random import randint\n",
    "\n",
    "import time\n",
    "\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the word list!\n",
      "Loaded the word vectors!\n",
      "400000\n",
      "(400000, 50)\n"
     ]
    }
   ],
   "source": [
    "wordsList = np.load('./data/wordsList.npy')\n",
    "print('Loaded the word list!')\n",
    "wordsList = wordsList.tolist() #Originally loaded as numpy array\n",
    "wordsList = [word.decode('UTF-8') for word in wordsList] #Encode words as UTF-8\n",
    "wordVectors = np.load('./data/wordVectors.npy')\n",
    "print ('Loaded the word vectors!')\n",
    "print(len(wordsList))\n",
    "print(wordVectors.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "array([-1.9327  ,  1.0421  , -0.78515 ,  0.91033 ,  0.22711 , -0.62158 ,\n       -1.6493  ,  0.07686 , -0.5868  ,  0.058831,  0.35628 ,  0.68916 ,\n       -0.50598 ,  0.70473 ,  1.2664  , -0.40031 , -0.020687,  0.80863 ,\n       -0.90566 , -0.074054, -0.87675 , -0.6291  , -0.12685 ,  0.11524 ,\n       -0.55685 , -1.6826  , -0.26291 ,  0.22632 ,  0.713   , -1.0828  ,\n        2.1231  ,  0.49869 ,  0.066711, -0.48226 , -0.17897 ,  0.47699 ,\n        0.16384 ,  0.16537 , -0.11506 , -0.15962 , -0.94926 , -0.42833 ,\n       -0.59457 ,  1.3566  , -0.27506 ,  0.19918 , -0.36008 ,  0.55667 ,\n       -0.70315 ,  0.17157 ], dtype=float32)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseballIndex = wordsList.index('baseball')\n",
    "wordVectors[baseballIndex]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "[    41    804 201534   1005     15   7446      5  13767      0      0]\n"
     ]
    }
   ],
   "source": [
    "maxSeqLength = 10 #Maximum length of sentence\n",
    "numDimensions = 300 #Dimensions for each word vector\n",
    "firstSentence = np.zeros((maxSeqLength), dtype='int32')\n",
    "firstSentence[0] = wordsList.index(\"i\")\n",
    "firstSentence[1] = wordsList.index(\"thought\")\n",
    "firstSentence[2] = wordsList.index(\"the\")\n",
    "firstSentence[3] = wordsList.index(\"movie\")\n",
    "firstSentence[4] = wordsList.index(\"was\")\n",
    "firstSentence[5] = wordsList.index(\"incredible\")\n",
    "firstSentence[6] = wordsList.index(\"and\")\n",
    "firstSentence[7] = wordsList.index(\"inspiring\")\n",
    "#firstSentence[8] and firstSentence[9] are going to be 0\n",
    "print(firstSentence.shape)\n",
    "print(firstSentence) #Shows the row index for each word"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 50)\n"
     ]
    }
   ],
   "source": [
    "with tf.compat.v1.Session() as sess:\n",
    "    print(tf.nn.embedding_lookup(wordVectors,firstSentence).eval().shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def getTrainBatch():\n",
    "\n",
    "    labels = []\n",
    "\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "\n",
    "    for i in range(batchSize):\n",
    "\n",
    "        if (i % 2) == 0:\n",
    "\n",
    "            num = randint(1, 11499)\n",
    "\n",
    "            labels.append([1, 0])\n",
    "\n",
    "        else:\n",
    "\n",
    "            num = randint(13499, 24999)\n",
    "\n",
    "            labels.append([0, 1])\n",
    "\n",
    "        arr[i] = ids[num - 1:num]\n",
    "\n",
    "    return arr, labels\n",
    "\n",
    "\n",
    "def getTestBatch():\n",
    "\n",
    "    labels = []\n",
    "\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "\n",
    "    for i in range(batchSize):\n",
    "\n",
    "        num = randint(11499, 13499)\n",
    "\n",
    "        if num <= 12499:\n",
    "\n",
    "            labels.append([1, 0])\n",
    "\n",
    "        else:\n",
    "\n",
    "            labels.append([0, 1])\n",
    "\n",
    "        arr[i] = ids[num - 1:num]\n",
    "\n",
    "    return arr, labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "\n",
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean) #平均值\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var-mean)))\n",
    "        tf.summary.scalar('stddev', stddev) #标准差\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the word list!\n",
      "Loaded the word vectors!\n",
      "400000\n",
      "(400000, 50)\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-10-f87c0d16b00b>:40: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-10-f87c0d16b00b>:44: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From D:\\dev\\install\\Anaconda3\\envs\\TF1.0\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From D:\\dev\\install\\Anaconda3\\envs\\TF1.0\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From <ipython-input-10-f87c0d16b00b>:64: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "iteration 1000/50000... loss 0.7658755779266357... accuracy 0.5...\n",
      "iteration 2000/50000... loss 0.6635794043540955... accuracy 0.5416666865348816...\n",
      "iteration 3000/50000... loss 0.6877812743186951... accuracy 0.6666666865348816...\n",
      "iteration 4000/50000... loss 0.6978812217712402... accuracy 0.375...\n",
      "iteration 5000/50000... loss 0.6270164847373962... accuracy 0.6666666865348816...\n",
      "iteration 6000/50000... loss 0.7677556872367859... accuracy 0.7083333134651184...\n",
      "iteration 7000/50000... loss 0.6457890272140503... accuracy 0.6666666865348816...\n",
      "iteration 8000/50000... loss 0.6092538237571716... accuracy 0.6666666865348816...\n",
      "iteration 9000/50000... loss 0.4846501648426056... accuracy 0.8333333134651184...\n",
      "iteration 10000/50000... loss 0.6565939784049988... accuracy 0.75...\n",
      "saved to models/pretrained_lstm.ckpt-10000\n",
      "iteration 11000/50000... loss 0.3991002142429352... accuracy 0.7916666865348816...\n",
      "iteration 12000/50000... loss 0.4247340261936188... accuracy 0.7916666865348816...\n",
      "iteration 13000/50000... loss 0.33860909938812256... accuracy 0.7916666865348816...\n",
      "iteration 14000/50000... loss 0.37258806824684143... accuracy 0.8333333134651184...\n",
      "iteration 15000/50000... loss 0.47098782658576965... accuracy 0.75...\n",
      "iteration 16000/50000... loss 0.2718977630138397... accuracy 0.9166666865348816...\n",
      "iteration 17000/50000... loss 0.3440638780593872... accuracy 0.9166666865348816...\n",
      "iteration 18000/50000... loss 0.34934625029563904... accuracy 0.8333333134651184...\n",
      "iteration 19000/50000... loss 0.25249040126800537... accuracy 0.9166666865348816...\n",
      "iteration 20000/50000... loss 0.19855844974517822... accuracy 0.9166666865348816...\n",
      "saved to models/pretrained_lstm.ckpt-20000\n",
      "iteration 21000/50000... loss 0.24097050726413727... accuracy 0.9583333134651184...\n",
      "iteration 22000/50000... loss 0.22797703742980957... accuracy 0.9583333134651184...\n",
      "iteration 23000/50000... loss 0.16180621087551117... accuracy 0.875...\n",
      "iteration 24000/50000... loss 0.14725154638290405... accuracy 0.9166666865348816...\n",
      "iteration 25000/50000... loss 0.3463806211948395... accuracy 0.875...\n",
      "iteration 26000/50000... loss 0.15939153730869293... accuracy 0.9166666865348816...\n",
      "iteration 27000/50000... loss 0.13056986033916473... accuracy 1.0...\n",
      "iteration 28000/50000... loss 0.07420369237661362... accuracy 1.0...\n",
      "iteration 29000/50000... loss 0.1339881271123886... accuracy 0.9166666865348816...\n",
      "iteration 30000/50000... loss 0.1823187619447708... accuracy 0.9166666865348816...\n",
      "saved to models/pretrained_lstm.ckpt-30000\n",
      "iteration 31000/50000... loss 0.04769900068640709... accuracy 0.9583333134651184...\n",
      "iteration 32000/50000... loss 0.12477586418390274... accuracy 0.9166666865348816...\n",
      "iteration 33000/50000... loss 0.1517522931098938... accuracy 0.9166666865348816...\n",
      "iteration 34000/50000... loss 0.3817426860332489... accuracy 0.9583333134651184...\n",
      "iteration 35000/50000... loss 0.1468001753091812... accuracy 0.9583333134651184...\n",
      "iteration 36000/50000... loss 0.05571834370493889... accuracy 1.0...\n",
      "iteration 37000/50000... loss 0.1619749218225479... accuracy 0.9583333134651184...\n",
      "iteration 38000/50000... loss 0.012185410596430302... accuracy 1.0...\n",
      "iteration 39000/50000... loss 0.043729763478040695... accuracy 0.9583333134651184...\n",
      "iteration 40000/50000... loss 0.013397946953773499... accuracy 1.0...\n",
      "saved to models/pretrained_lstm.ckpt-40000\n",
      "iteration 41000/50000... loss 0.009377644397318363... accuracy 1.0...\n",
      "iteration 42000/50000... loss 0.07573241740465164... accuracy 1.0...\n",
      "iteration 43000/50000... loss 0.009282621555030346... accuracy 1.0...\n",
      "iteration 44000/50000... loss 0.04867282509803772... accuracy 1.0...\n",
      "iteration 45000/50000... loss 0.03136777505278587... accuracy 1.0...\n",
      "iteration 46000/50000... loss 0.008126088418066502... accuracy 1.0...\n",
      "iteration 47000/50000... loss 0.020738510414958... accuracy 1.0...\n",
      "iteration 48000/50000... loss 0.0235008392482996... accuracy 1.0...\n",
      "iteration 49000/50000... loss 0.05830908939242363... accuracy 1.0...\n",
      "iteration 50000/50000... loss 0.05242060124874115... accuracy 0.9583333134651184...\n",
      "saved to models/pretrained_lstm.ckpt-50000\n"
     ]
    }
   ],
   "source": [
    "maxSeqLength = 250\n",
    "\n",
    "batchSize = 24  # 批处理大小\n",
    "\n",
    "numDimensions = 250\n",
    "\n",
    "lstmUnits = 64  # LSTM的单元个数\n",
    "\n",
    "numClasses = 2  # 分类类别\n",
    "\n",
    "iterations = 50000  # 训练次数\n",
    "\n",
    "wordsList = np.load(r'.\\data\\wordsList.npy')\n",
    "print('Loaded the word list!')\n",
    "wordsList = wordsList.tolist()  #\n",
    "\n",
    "wordsList = [word.decode('UTF-8') for word in wordsList]  # 不然会报word not in vocab错误\n",
    "\n",
    "wordVectors = np.load(r'.\\data\\wordVectors.npy')\n",
    "print('Loaded the word vectors!')\n",
    "print(len(wordsList))\n",
    "print(wordVectors.shape)\n",
    "\n",
    "ids = np.load(r'.\\data\\idsMatrix.npy')\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.name_scope(\"input\"):\n",
    "    input_data = tf.placeholder(tf.int32, shape=[batchSize, maxSeqLength], name=\"x_input\")  # 占位符，必不可少\n",
    "    labels = tf.placeholder(tf.int32, shape=[batchSize, numClasses], name=\"y_input\")\n",
    "\n",
    "data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]), dtype=tf.float32)\n",
    "\n",
    "data = tf.nn.embedding_lookup(wordVectors, input_data)\n",
    "\n",
    "data = tf.cast(data, tf.float32)  # 由于版本的问题，这一步必不可少，将x的数据格式转化成dtype，有的版本可以不写\n",
    "\n",
    "'''LSTM网络的构建'''\n",
    "\n",
    "lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "\n",
    "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "\n",
    "value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)\n",
    "\n",
    "with tf.name_scope(\"lstm\"):\n",
    "    with tf.name_scope(\"weight\"):\n",
    "        weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "        variable_summaries(weight)\n",
    "    with tf.name_scope(\"bias\"):\n",
    "        bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "        variable_summaries(bias)\n",
    "    value = tf.transpose(value, [1, 0, 2])  # value的输出为[batchsize,length,hidden_size]\n",
    "\n",
    "    last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "    with tf.name_scope(\"wx_plus_b\"):\n",
    "        prediction = (tf.matmul(last, weight) + bias)\n",
    "\n",
    "correctPred = tf.equal(tf.argmax(prediction, 1), tf.argmax(labels, 1))\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "with tf.name_scope(\"loss\"):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))  # 计算交叉熵\n",
    "    tf.summary.scalar('loss', loss)\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)  # 随机梯度下降最小化loss\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    merged=tf.summary.merge_all()#合并所有的summary data的获取函数，merge_all 可以将所有summary全部保存到磁盘，以便tensorboard显示。如果没有特殊要求，一般用这一句就可一显示训练时的各种信息了。\n",
    "    writer=tf.summary.FileWriter(\"./tensorboard/test1/\",sess.graph)\n",
    "\n",
    "    for i in range(iterations):\n",
    "\n",
    "        # Next Batch of reviews\n",
    "\n",
    "        nextBatch, nextBatchLabels = getTrainBatch()\n",
    "\n",
    "        sess.run(optimizer, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "        rs=sess.run(merged,feed_dict={input_data: nextBatch, labels: nextBatchLabels})#运行所有合并所有的图，获取summary data函数节点和graph是独立的，调用的时候也需要运行session\n",
    "        writer.add_summary(rs,i)\n",
    "        if (i + 1) % 1000 == 0 and i != 0:\n",
    "\n",
    "            loss_ = sess.run(loss, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "\n",
    "            accuracy_ = sess.run(accuracy, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "\n",
    "            print(\"iteration {}/{}...\".format(i + 1, iterations),\n",
    "\n",
    "                  \"loss {}...\".format(loss_),\n",
    "\n",
    "                  \"accuracy {}...\".format(accuracy_))\n",
    "\n",
    "        if (i + 1) % 10000 == 0 and i != 0:\n",
    "            save_path = saver.save(sess, \"models/pretrained_lstm.ckpt\", global_step=i + 1)\n",
    "            print(\"saved to %s\" % save_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models\\pretrained_lstm.ckpt-50000\n",
      "Accuracy for this batch: 95.83333134651184\n",
      "Accuracy for this batch: 95.83333134651184\n",
      "Accuracy for this batch: 83.33333134651184\n",
      "Accuracy for this batch: 70.83333134651184\n",
      "Accuracy for this batch: 83.33333134651184\n",
      "Accuracy for this batch: 79.16666865348816\n",
      "Accuracy for this batch: 75.0\n",
      "Accuracy for this batch: 79.16666865348816\n",
      "Accuracy for this batch: 100.0\n",
      "Accuracy for this batch: 91.66666865348816\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, tf.train.latest_checkpoint('models'))\n",
    "iterations = 10\n",
    "for i in range(iterations):\n",
    "    nextBatch, nextBatchLabels = getTestBatch()\n",
    "    print(\"Accuracy for this batch:\", (sess.run(accuracy, {input_data: nextBatch, labels: nextBatchLabels})) * 100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}