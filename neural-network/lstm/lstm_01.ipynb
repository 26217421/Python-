{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from random import randint\n",
    "\n",
    "import time\n",
    "\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the word list!\n",
      "Loaded the word vectors!\n",
      "400000\n",
      "(400000, 50)\n"
     ]
    }
   ],
   "source": [
    "wordsList = np.load('./data/wordsList.npy')\n",
    "print('Loaded the word list!')\n",
    "wordsList = wordsList.tolist() #Originally loaded as numpy array\n",
    "wordsList = [word.decode('UTF-8') for word in wordsList] #Encode words as UTF-8\n",
    "wordVectors = np.load('./data/wordVectors.npy')\n",
    "print ('Loaded the word vectors!')\n",
    "print(len(wordsList))\n",
    "print(wordVectors.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "array([-1.9327  ,  1.0421  , -0.78515 ,  0.91033 ,  0.22711 , -0.62158 ,\n       -1.6493  ,  0.07686 , -0.5868  ,  0.058831,  0.35628 ,  0.68916 ,\n       -0.50598 ,  0.70473 ,  1.2664  , -0.40031 , -0.020687,  0.80863 ,\n       -0.90566 , -0.074054, -0.87675 , -0.6291  , -0.12685 ,  0.11524 ,\n       -0.55685 , -1.6826  , -0.26291 ,  0.22632 ,  0.713   , -1.0828  ,\n        2.1231  ,  0.49869 ,  0.066711, -0.48226 , -0.17897 ,  0.47699 ,\n        0.16384 ,  0.16537 , -0.11506 , -0.15962 , -0.94926 , -0.42833 ,\n       -0.59457 ,  1.3566  , -0.27506 ,  0.19918 , -0.36008 ,  0.55667 ,\n       -0.70315 ,  0.17157 ], dtype=float32)"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseballIndex = wordsList.index('baseball')\n",
    "wordVectors[baseballIndex]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "[    41    804 201534   1005     15   7446      5  13767      0      0]\n"
     ]
    }
   ],
   "source": [
    "maxSeqLength = 10 #Maximum length of sentence\n",
    "numDimensions = 300 #Dimensions for each word vector\n",
    "firstSentence = np.zeros((maxSeqLength), dtype='int32')\n",
    "firstSentence[0] = wordsList.index(\"i\")\n",
    "firstSentence[1] = wordsList.index(\"thought\")\n",
    "firstSentence[2] = wordsList.index(\"the\")\n",
    "firstSentence[3] = wordsList.index(\"movie\")\n",
    "firstSentence[4] = wordsList.index(\"was\")\n",
    "firstSentence[5] = wordsList.index(\"incredible\")\n",
    "firstSentence[6] = wordsList.index(\"and\")\n",
    "firstSentence[7] = wordsList.index(\"inspiring\")\n",
    "#firstSentence[8] and firstSentence[9] are going to be 0\n",
    "print(firstSentence.shape)\n",
    "print(firstSentence) #Shows the row index for each word"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 50)\n"
     ]
    }
   ],
   "source": [
    "with tf.compat.v1.Session() as sess:\n",
    "    print(tf.nn.embedding_lookup(wordVectors,firstSentence).eval().shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "def getTrainBatch():\n",
    "\n",
    "    labels = []\n",
    "\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "\n",
    "    for i in range(batchSize):\n",
    "\n",
    "        if (i % 2) == 0:\n",
    "\n",
    "            num = randint(1, 11499)\n",
    "\n",
    "            labels.append([1, 0])\n",
    "\n",
    "        else:\n",
    "\n",
    "            num = randint(13499, 24999)\n",
    "\n",
    "            labels.append([0, 1])\n",
    "\n",
    "        arr[i] = ids[num - 1:num]\n",
    "\n",
    "    return arr, labels\n",
    "\n",
    "\n",
    "def getTestBatch():\n",
    "\n",
    "    labels = []\n",
    "\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "\n",
    "    for i in range(batchSize):\n",
    "\n",
    "        num = randint(11499, 13499)\n",
    "\n",
    "        if num <= 12499:\n",
    "\n",
    "            labels.append([1, 0])\n",
    "\n",
    "        else:\n",
    "\n",
    "            labels.append([0, 1])\n",
    "\n",
    "        arr[i] = ids[num - 1:num]\n",
    "\n",
    "    return arr, labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "\n",
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean) #平均值\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var-mean)))\n",
    "        tf.summary.scalar('stddev', stddev) #标准差\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the word list!\n",
      "Loaded the word vectors!\n",
      "400000\n",
      "(400000, 50)\n",
      "iteration 1000/50000... loss 0.6836003661155701... accuracy 0.5...\n",
      "iteration 2000/50000... loss 0.6980269551277161... accuracy 0.5833333134651184...\n",
      "iteration 3000/50000... loss 0.6482793688774109... accuracy 0.5416666865348816...\n",
      "iteration 4000/50000... loss 0.6710816025733948... accuracy 0.5...\n",
      "iteration 5000/50000... loss 0.4527755081653595... accuracy 0.8333333134651184...\n",
      "iteration 6000/50000... loss 0.40605247020721436... accuracy 0.8333333134651184...\n",
      "iteration 7000/50000... loss 0.49711310863494873... accuracy 0.7916666865348816...\n",
      "iteration 8000/50000... loss 0.30767926573753357... accuracy 0.875...\n",
      "iteration 9000/50000... loss 0.44519898295402527... accuracy 0.8333333134651184...\n",
      "iteration 10000/50000... loss 0.691591739654541... accuracy 0.75...\n",
      "saved to models/pretrained_lstm.ckpt-10000\n",
      "iteration 11000/50000... loss 0.25381508469581604... accuracy 0.8333333134651184...\n",
      "iteration 12000/50000... loss 0.35043320059776306... accuracy 0.7916666865348816...\n",
      "iteration 13000/50000... loss 0.28300872445106506... accuracy 0.8333333134651184...\n",
      "iteration 14000/50000... loss 0.12959744036197662... accuracy 1.0...\n",
      "iteration 15000/50000... loss 0.3093579113483429... accuracy 0.875...\n",
      "iteration 16000/50000... loss 0.2569023072719574... accuracy 0.875...\n",
      "iteration 17000/50000... loss 0.07334151118993759... accuracy 1.0...\n",
      "iteration 18000/50000... loss 0.1041187271475792... accuracy 0.9583333134651184...\n",
      "iteration 19000/50000... loss 0.2575145661830902... accuracy 0.875...\n",
      "iteration 20000/50000... loss 0.058753203600645065... accuracy 1.0...\n",
      "saved to models/pretrained_lstm.ckpt-20000\n",
      "iteration 21000/50000... loss 0.14565198123455048... accuracy 0.9583333134651184...\n",
      "iteration 22000/50000... loss 0.038812555372714996... accuracy 1.0...\n",
      "iteration 23000/50000... loss 0.02094328962266445... accuracy 0.9583333134651184...\n",
      "iteration 24000/50000... loss 0.035318128764629364... accuracy 1.0...\n",
      "iteration 25000/50000... loss 0.06361538171768188... accuracy 0.9583333134651184...\n",
      "iteration 26000/50000... loss 0.059139419347047806... accuracy 0.9583333134651184...\n",
      "iteration 27000/50000... loss 0.07521089166402817... accuracy 1.0...\n",
      "iteration 28000/50000... loss 0.017827413976192474... accuracy 1.0...\n",
      "iteration 29000/50000... loss 0.06215597689151764... accuracy 0.9583333134651184...\n",
      "iteration 30000/50000... loss 0.04421323910355568... accuracy 0.9583333134651184...\n",
      "saved to models/pretrained_lstm.ckpt-30000\n",
      "iteration 31000/50000... loss 0.010246297344565392... accuracy 1.0...\n",
      "iteration 32000/50000... loss 0.009554541669785976... accuracy 1.0...\n",
      "iteration 33000/50000... loss 0.023101896047592163... accuracy 1.0...\n",
      "iteration 34000/50000... loss 0.018996641039848328... accuracy 1.0...\n",
      "iteration 35000/50000... loss 0.028018079698085785... accuracy 1.0...\n",
      "iteration 36000/50000... loss 0.0036241356283426285... accuracy 1.0...\n",
      "iteration 37000/50000... loss 0.010996800847351551... accuracy 1.0...\n",
      "iteration 38000/50000... loss 0.007738593965768814... accuracy 1.0...\n",
      "iteration 39000/50000... loss 0.003617196576669812... accuracy 1.0...\n",
      "iteration 40000/50000... loss 0.03679179772734642... accuracy 1.0...\n",
      "saved to models/pretrained_lstm.ckpt-40000\n",
      "iteration 41000/50000... loss 0.007140284404158592... accuracy 1.0...\n",
      "iteration 42000/50000... loss 0.005494088400155306... accuracy 1.0...\n",
      "iteration 43000/50000... loss 0.011027020402252674... accuracy 1.0...\n",
      "iteration 44000/50000... loss 0.008113685064017773... accuracy 1.0...\n",
      "iteration 45000/50000... loss 0.01297266036272049... accuracy 1.0...\n",
      "iteration 46000/50000... loss 0.007143261376768351... accuracy 1.0...\n",
      "iteration 47000/50000... loss 0.2431858628988266... accuracy 0.9583333134651184...\n",
      "iteration 48000/50000... loss 0.0048119849525392056... accuracy 1.0...\n",
      "iteration 49000/50000... loss 0.10824647545814514... accuracy 0.9583333134651184...\n",
      "iteration 50000/50000... loss 0.004552870523184538... accuracy 1.0...\n",
      "saved to models/pretrained_lstm.ckpt-50000\n"
     ]
    }
   ],
   "source": [
    "maxSeqLength = 250\n",
    "\n",
    "batchSize = 24  # 批处理大小\n",
    "\n",
    "numDimensions = 250\n",
    "\n",
    "lstmUnits = 64  # LSTM的单元个数\n",
    "\n",
    "numClasses = 2  # 分类类别\n",
    "\n",
    "iterations = 50000  # 训练次数\n",
    "\n",
    "wordsList = np.load(r'.\\data\\wordsList.npy')\n",
    "print('Loaded the word list!')\n",
    "wordsList = wordsList.tolist()  #\n",
    "\n",
    "wordsList = [word.decode('UTF-8') for word in wordsList]  # 不然会报word not in vocab错误\n",
    "\n",
    "wordVectors = np.load(r'.\\data\\wordVectors.npy')\n",
    "print('Loaded the word vectors!')\n",
    "print(len(wordsList))\n",
    "print(wordVectors.shape)\n",
    "\n",
    "ids = np.load(r'.\\data\\idsMatrix.npy')\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.name_scope(\"input\"):\n",
    "    input_data = tf.placeholder(tf.int32, shape=[batchSize, maxSeqLength], name=\"x_input\")  # 占位符，必不可少\n",
    "    labels = tf.placeholder(tf.int32, shape=[batchSize, numClasses], name=\"y_input\")\n",
    "\n",
    "data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]), dtype=tf.float32)\n",
    "\n",
    "data = tf.nn.embedding_lookup(wordVectors, input_data)\n",
    "\n",
    "data = tf.cast(data, tf.float32)  # 由于版本的问题，这一步必不可少，将x的数据格式转化成dtype，有的版本可以不写\n",
    "\n",
    "'''LSTM网络的构建'''\n",
    "\n",
    "lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "\n",
    "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "\n",
    "value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)\n",
    "\n",
    "with tf.name_scope(\"lstm\"):\n",
    "    with tf.name_scope(\"weight\"):\n",
    "        weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "        variable_summaries(weight)\n",
    "    with tf.name_scope(\"bias\"):\n",
    "        bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "        variable_summaries(bias)\n",
    "    value = tf.transpose(value, [1, 0, 2])  # value的输出为[batchsize,length,hidden_size]\n",
    "\n",
    "    last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "    with tf.name_scope(\"wx_plus_b\"):\n",
    "        prediction = (tf.matmul(last, weight) + bias)\n",
    "\n",
    "correctPred = tf.equal(tf.argmax(prediction, 1), tf.argmax(labels, 1))\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "with tf.name_scope(\"loss\"):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))  # 计算交叉熵\n",
    "    tf.summary.scalar('loss', loss)\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.0015).minimize(loss)  # 随机梯度下降最小化loss\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    merged=tf.summary.merge_all()#合并所有的summary data的获取函数，merge_all 可以将所有summary全部保存到磁盘，以便tensorboard显示。如果没有特殊要求，一般用这一句就可一显示训练时的各种信息了。\n",
    "    writer=tf.summary.FileWriter(\"./tensorboard/test1/\",sess.graph)\n",
    "\n",
    "    for i in range(iterations):\n",
    "\n",
    "        # Next Batch of reviews\n",
    "\n",
    "        nextBatch, nextBatchLabels = getTrainBatch()\n",
    "\n",
    "        sess.run(optimizer, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "        rs=sess.run(merged,feed_dict={input_data: nextBatch, labels: nextBatchLabels})#运行所有合并所有的图，获取summary data函数节点和graph是独立的，调用的时候也需要运行session\n",
    "        writer.add_summary(rs,i)\n",
    "        if (i + 1) % 1000 == 0 and i != 0:\n",
    "\n",
    "            loss_ = sess.run(loss, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "\n",
    "            accuracy_ = sess.run(accuracy, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "\n",
    "            print(\"iteration {}/{}...\".format(i + 1, iterations),\n",
    "\n",
    "                  \"loss {}...\".format(loss_),\n",
    "\n",
    "                  \"accuracy {}...\".format(accuracy_))\n",
    "\n",
    "        if (i + 1) % 10000 == 0 and i != 0:\n",
    "            save_path = saver.save(sess, \"models/pretrained_lstm.ckpt\", global_step=i + 1)\n",
    "            print(\"saved to %s\" % save_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models\\pretrained_lstm.ckpt-50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\dev\\install\\Anaconda3\\envs\\TF1.0\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for this batch: 70.83333134651184\n",
      "Accuracy for this batch: 83.33333134651184\n",
      "Accuracy for this batch: 87.5\n",
      "Accuracy for this batch: 79.16666865348816\n",
      "Accuracy for this batch: 83.33333134651184\n",
      "Accuracy for this batch: 83.33333134651184\n",
      "Accuracy for this batch: 95.83333134651184\n",
      "Accuracy for this batch: 83.33333134651184\n",
      "Accuracy for this batch: 83.33333134651184\n",
      "Accuracy for this batch: 70.83333134651184\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, tf.train.latest_checkpoint('models'))\n",
    "iterations = 10\n",
    "for i in range(iterations):\n",
    "    nextBatch, nextBatchLabels = getTestBatch()\n",
    "    print(\"Accuracy for this batch:\", (sess.run(accuracy, {input_data: nextBatch, labels: nextBatchLabels})) * 100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}